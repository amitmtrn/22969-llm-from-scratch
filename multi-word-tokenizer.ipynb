{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eOS-JntYUQ56",
   "metadata": {
    "id": "eOS-JntYUQ56"
   },
   "source": [
    "# Multi-Word Tokenizer Lab\n",
    "\n",
    "This notebook implements a tokenizer that treats specific multi-word phrases as single tokens and compares its performance against the standard GPT-2 BPE tokenizer using the OpenWebText dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "DY5_ex6UUQ57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3260,
     "status": "ok",
     "timestamp": 1771100412080,
     "user": {
      "displayName": "Amit Avigad",
      "userId": "05878735534699042778"
     },
     "user_tz": -120
    },
    "id": "DY5_ex6UUQ57",
    "outputId": "89966d5f-dbbb-4bbf-fa3b-1d73660622a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.9/site-packages (4.5.0)\n",
      "Requirement already satisfied: tiktoken in ./.venv/lib/python3.9/site-packages (0.12.0)\n",
      "Requirement already satisfied: regex in ./.venv/lib/python3.9/site-packages (2026.1.15)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.9/site-packages (2.32.5)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.9/site-packages (2.8.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (4.67.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.venv/lib/python3.9/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.venv/lib/python3.9/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.9/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in ./.venv/lib/python3.9/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.9/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.venv/lib/python3.9/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in ./.venv/lib/python3.9/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in ./.venv/lib/python3.9/site-packages (from datasets) (1.4.1)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.9/site-packages (from datasets) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.9/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.9/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.9/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.9/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.9/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.9/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in ./.venv/lib/python3.9/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in ./.venv/lib/python3.9/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests) (2.6.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.9/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.9/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.9/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: typer>=0.23.1 in ./.venv/lib/python3.9/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (0.23.1)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.9/site-packages (from typer>=0.23.1->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.1.8)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.9/site-packages (from typer>=0.23.1->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (14.3.2)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in ./.venv/lib/python3.9/site-packages (from typer>=0.23.1->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.9/site-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.9/site-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.1->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (0.1.2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install necessary libraries if not present\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "packages = [\"datasets\", \"tiktoken\", \"regex\", \"requests\", \"torch\", \"tqdm\"]\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "xp1qnwPjUQ58",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1771100412090,
     "user": {
      "displayName": "Amit Avigad",
      "userId": "05878735534699042778"
     },
     "user_tz": -120
    },
    "id": "xp1qnwPjUQ58"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from datasets import load_dataset\n",
    "from collections import Counter, defaultdict\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OC_CflJ4UQ58",
   "metadata": {
    "id": "OC_CflJ4UQ58"
   },
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "We use `Skylion007/openwebtext` (WebText replication) via streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "E17CsM8dUQ59",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99,
     "referenced_widgets": [
      "4e7414fdef224686899b169e39989ab4",
      "ff6f1ef995a24e8f882c8468fc34bbcb",
      "6f05ae4301854137bd61974cc2207f74",
      "7f7e5f4d70cb438d86f85343b8552fcd",
      "42c80d5039204544b42181e6df59cf9f",
      "7e149ea2387442e7a8952b01cd6d13a9",
      "8bd139c41a134a88865dbbda5b23418c",
      "4154dc6b5b1747b5b3e52aea584c3ea4",
      "b35f8cd7055e418e9c0827f61441f169",
      "ade80f64e90e4af7a88abcdcb58ced65",
      "16c8aa2582a34cd1b241c54bbe2eaa74",
      "6e8d7a70f8ae41898cf368b06d6a3c60",
      "72b981f9343b4fb6ae100558e7c08eda",
      "5880685d88e44159a4d539491871b702",
      "b770fd15ef5b441ebdf80b708939fb90",
      "38d029a60a8b470db487060872a4b096",
      "7185eb94c0e540db88a713cca0a36d0a",
      "8ae55dbcb45b4f63a345f92d422079a5",
      "7ebff1e554da4a9aae4a7c63494966f2",
      "bf3602a4fcd34913ac8e0e7206694b23",
      "11e9f419acb64d7a978fbea8424d79e4",
      "415d2664f86a45e296c77456cb1c8c98"
     ]
    },
    "executionInfo": {
     "elapsed": 964,
     "status": "ok",
     "timestamp": 1771100413053,
     "user": {
      "displayName": "Amit Avigad",
      "userId": "05878735534699042778"
     },
     "user_tz": -120
    },
    "id": "E17CsM8dUQ59",
    "outputId": "b8f5469d-0567-45d8-eacc-d362fb88fe21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset (streaming)...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset (streaming)...\")\n",
    "dataset = load_dataset(\"Skylion007/openwebtext\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WIHOQDgeUQ59",
   "metadata": {
    "id": "WIHOQDgeUQ59"
   },
   "source": [
    "## 3. Multi-Word Tokenizer Implementation\n",
    "\n",
    "This tokenizer will:\n",
    "1.  Identify specific multi-word phrases in the text.\n",
    "2.  Tokenize them as single units.\n",
    "3.  Delegate the rest to the base tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "IueMQBIwUQ59",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1771100413057,
     "user": {
      "displayName": "Amit Avigad",
      "userId": "05878735534699042778"
     },
     "user_tz": -120
    },
    "id": "IueMQBIwUQ59"
   },
   "outputs": [],
   "source": [
    "class MultiWordTokenizer:\n",
    "    def __init__(self, phrases):\n",
    "        \"\"\"\n",
    "        phrases: list of strings (e.g., [\"New York\", \"machine learning\"])\n",
    "        \"\"\"\n",
    "        self.base_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        self.base_vocab_size = self.base_tokenizer.n_vocab\n",
    "\n",
    "        # Mappings for custom phrase tokens\n",
    "        self.phrase_to_id = {}\n",
    "        self.id_to_phrase = {}\n",
    "\n",
    "        next_id = self.base_vocab_size\n",
    "\n",
    "        # Flattened list for regex\n",
    "        all_phrases = []\n",
    "\n",
    "        for phrase in phrases:\n",
    "            # Register original phrase\n",
    "            self.phrase_to_id[phrase] = next_id\n",
    "            self.id_to_phrase[next_id] = phrase\n",
    "            all_phrases.append(phrase)\n",
    "            next_id += 1\n",
    "\n",
    "            # Register phrase with leading space (CRITICAL for BPE efficiency)\n",
    "            # This prevents breaking \" word\" tokens into \" \" + \"word\"\n",
    "            space_phrase = \" \" + phrase\n",
    "            self.phrase_to_id[space_phrase] = next_id\n",
    "            self.id_to_phrase[next_id] = space_phrase\n",
    "            all_phrases.append(space_phrase)\n",
    "            next_id += 1\n",
    "\n",
    "        self.vocab_size = next_id\n",
    "\n",
    "        # Create a regex pattern to match phrases first\n",
    "        # Sort by length (descending) to match longest phrases first (e.g. \" New York\" before \"New York\")\n",
    "        sorted_phrases = sorted(all_phrases, key=len, reverse=True)\n",
    "        self.phrase_pattern = re.compile(\"|\".join(map(re.escape, sorted_phrases)))\n",
    "\n",
    "    def encode(self, text, allowed_special=None):\n",
    "        # Split text by phrases\n",
    "        tokens = []\n",
    "        last_end = 0\n",
    "\n",
    "        # tiktoken encode needs allowed_special={'<|endoftext|>'} if present\n",
    "        special = {'<|endoftext|>'} if allowed_special is None else allowed_special\n",
    "\n",
    "        for match in self.phrase_pattern.finditer(text):\n",
    "            start, end = match.span()\n",
    "\n",
    "            # Process text before the match with base tokenizer\n",
    "            if start > last_end:\n",
    "                pre_text = text[last_end:start]\n",
    "                # If pre_text ends with a space but we matched a non-space phrase, \n",
    "                # we might still be splitting \"word \" + \"phrase\". \n",
    "                # But since we prioritized \" phrase\", this is less likely to happen for the phrase itself.\n",
    "                tokens.extend(self.base_tokenizer.encode(pre_text, allowed_special=special))\n",
    "\n",
    "            # Add the phrase token\n",
    "            phrase = match.group()\n",
    "            tokens.append(self.phrase_to_id[phrase])\n",
    "\n",
    "            last_end = end\n",
    "\n",
    "        # Process remaining text\n",
    "        if last_end < len(text):\n",
    "            tokens.extend(self.base_tokenizer.encode(text[last_end:], allowed_special=special))\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # Split ids into chunks of (base_tokens) and (phrase_tokens)\n",
    "        decoded_parts = []\n",
    "        current_chunk = []\n",
    "\n",
    "        for id in ids:\n",
    "            if id < self.base_vocab_size:\n",
    "                current_chunk.append(id)\n",
    "            else:\n",
    "                # Decode accumulated base tokens\n",
    "                if current_chunk:\n",
    "                    decoded_parts.append(self.base_tokenizer.decode(current_chunk))\n",
    "                    current_chunk = []\n",
    "                # Decode phrase token\n",
    "                decoded_parts.append(self.id_to_phrase.get(id, \"\"))\n",
    "\n",
    "        # Decode final chunk\n",
    "        if current_chunk:\n",
    "             decoded_parts.append(self.base_tokenizer.decode(current_chunk))\n",
    "\n",
    "        return \"\".join(decoded_parts)\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        # Fake vocab dict for compatibility if needed\n",
    "        return {**{str(i):i for i in range(self.base_vocab_size)}, **self.phrase_to_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scK1Qd00UQ5-",
   "metadata": {
    "id": "scK1Qd00UQ5-"
   },
   "source": [
    "## 4. Streaming Vocab Building\n",
    "\n",
    "We use `tqdm` for progress tracking and implement periodic pruning to prevent Out-Of-Memory errors when processing a massive dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1foSfgXUQ5-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 251798,
     "status": "ok",
     "timestamp": 1771100664858,
     "user": {
      "displayName": "Amit Avigad",
      "userId": "05878735534699042778"
     },
     "user_tz": -120
    },
    "id": "b1foSfgXUQ5-",
    "outputId": "0ff4ab64-d3b6-405b-d81b-05ebf030a4d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting streaming phrase extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100000/100000 [01:39<00:00, 1000.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing. Documents processed: 100000\n",
      "Filtering phrases...\n",
      "Top 10 INTERESTING phrases (Actual Vocab): ['of the', 'in the', 'to the', 'on the', 'for the', 'to be', 'and the', 'at the', 'with the', 'in a']\n"
     ]
    }
   ],
   "source": [
    "def build_vocab_stream(dataset, limit=None):\n",
    "    \"\"\"\n",
    "    Builds phrase counts by streaming through the dataset.\n",
    "    (Base vocab is handled by tiktoken)\n",
    "    \"\"\"\n",
    "    phrase_counts_2 = Counter()\n",
    "    phrase_counts_3 = Counter()\n",
    "\n",
    "    MAX_PHRASE_ENTRIES = 500000\n",
    "\n",
    "    print(\"Starting streaming phrase extraction...\")\n",
    "    if limit:\n",
    "        pbar = tqdm.tqdm(total=limit)\n",
    "    else:\n",
    "        pbar = tqdm.tqdm(desc=\"Processing docs\")\n",
    "\n",
    "    i = 0\n",
    "    try:\n",
    "        for index, doc in enumerate(dataset):\n",
    "            if limit is not None and i >= limit:\n",
    "                break\n",
    "\n",
    "            text = doc['text']\n",
    "\n",
    "            # Phrases (n-grams)\n",
    "            # Use regex to find alphanumeric tokens, avoiding punctuation issues\n",
    "            words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "            words = [w for w in words if w.strip()]\n",
    "\n",
    "            if len(words) >= 2:\n",
    "                ngrams_2 = zip(*[words[i:] for i in range(2)])\n",
    "                phrase_counts_2.update(\" \".join(ngram) for ngram in ngrams_2)\n",
    "\n",
    "            if len(words) >= 3:\n",
    "                ngrams_3 = zip(*[words[i:] for i in range(3)])\n",
    "                phrase_counts_3.update(\" \".join(ngram) for ngram in ngrams_3)\n",
    "\n",
    "            if i % 10000 == 0 and i > 0:\n",
    "                 if len(phrase_counts_2) > MAX_PHRASE_ENTRIES:\n",
    "                     phrase_counts_2 = Counter({k: c for k, c in phrase_counts_2.items() if c > 1})\n",
    "                 if len(phrase_counts_3) > MAX_PHRASE_ENTRIES:\n",
    "                     phrase_counts_3 = Counter({k: c for k, c in phrase_counts_3.items() if c > 1})\n",
    "\n",
    "            i += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nInterrupted by user.\")\n",
    "\n",
    "    pbar.close()\n",
    "    print(f\"\\nFinished processing. Documents processed: {i}\")\n",
    "    return phrase_counts_2, phrase_counts_3\n",
    "\n",
    "\n",
    "# Set limit for testing.\n",
    "processed_limit = 100000\n",
    "\n",
    "phrases_2, phrases_3 = build_vocab_stream(dataset, limit=processed_limit)\n",
    "\n",
    "# --- FILTERING LOGIC ---\n",
    "# Expand stop words to catch more common glue phrases\n",
    "stop_words = {\n",
    "}\n",
    "\n",
    "def is_interesting(phrase):\n",
    "    parts = phrase.split()\n",
    "    # Check if all parts are stopwords (e.g., \"of the\", \"in a\")\n",
    "    if all(p.lower() in stop_words for p in parts):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "print(\"Filtering phrases...\")\n",
    "filtered_2 = Counter({p: c for p, c in phrases_2.items() if is_interesting(p)})\n",
    "filtered_3 = Counter({p: c for p, c in phrases_3.items() if is_interesting(p)})\n",
    "\n",
    "# Select top phrases from the FILTERED lists\n",
    "common_phrases = [p for p, c in filtered_2.most_common(5000)] + [p for p, c in filtered_3.most_common(2000)]\n",
    "\n",
    "print(f\"Top 10 INTERESTING phrases (Actual Vocab): {common_phrases[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "tLwCvYTjUQ5_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1771100664885,
     "user": {
      "displayName": "Amit Avigad",
      "userId": "05878735534699042778"
     },
     "user_tz": -120
    },
    "id": "tLwCvYTjUQ5_",
    "outputId": "42dbac81-08f5-49ea-d3e0-4f362e574d3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Word Tokenizer Total Vocab Size: 52257\n"
     ]
    }
   ],
   "source": [
    "# Initialize Tokenizers\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "mw_tokenizer = MultiWordTokenizer(common_phrases)\n",
    "\n",
    "# Tiktoken base vocab is ~50257. Our tokens start after that.\n",
    "print(f\"Multi-Word Tokenizer Total Vocab Size: {mw_tokenizer.vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2FmYj7X1UQ5_",
   "metadata": {
    "id": "2FmYj7X1UQ5_"
   },
   "source": [
    "## 5. Comparision Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "RhhbGf39UQ5_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1771100664894,
     "user": {
      "displayName": "Amit Avigad",
      "userId": "05878735534699042778"
     },
     "user_tz": -120
    },
    "id": "RhhbGf39UQ5_",
    "outputId": "f6aebf4d-9c80-43ef-e7ae-d8b839d5f71d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching a random sample from dataset...\n",
      "Sample Text Length: 5000 characters\n",
      "Sample snippet: If you live in Minneapolis\u2019 Powderhorn neighborhood, keep your eyes peeled on Aug. 14. Starting that...\n",
      "--- Multi-Word Tokenizer ---\n",
      "Token Count: 1100\n",
      "Compression Ratio: 4.55 chars/token\n",
      "--- GPT-2 BPE (tiktoken) ---\n",
      "Token Count: 1175\n",
      "Compression Ratio: 4.26 chars/token\n",
      "\n",
      "Difference: 75 tokens (6.38%)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_tokenizer(tokenizer, text, name):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    token_count = len(encoded)\n",
    "    chars = len(text)\n",
    "    compression_ratio = chars / token_count if token_count > 0 else 0\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(f\"Token Count: {token_count}\")\n",
    "    print(f\"Compression Ratio: {compression_ratio:.2f} chars/token\")\n",
    "    return token_count\n",
    "\n",
    "\n",
    "# Run comparison on a larger, random sample from the dataset\n",
    "print(\"Fetching a random sample from dataset...\")\n",
    "try:\n",
    "    # We use a new shuffled instance to get a random sample\n",
    "    # Using a different seed or buffer ensures some randomness\n",
    "    temp_ds = load_dataset(\"Skylion007/openwebtext\", split=\"train\", streaming=True).shuffle(seed=42, buffer_size=1000)\n",
    "    sample_doc = next(iter(temp_ds))\n",
    "    sample_text = sample_doc['text']\n",
    "\n",
    "    # Ensure it's substantial\n",
    "    if len(sample_text) < 1000:\n",
    "        # If too short, grab another\n",
    "        sample_doc = next(iter(temp_ds))\n",
    "        sample_text += \"\\n\\n\" + sample_doc['text']\n",
    "\n",
    "    # Truncate if too huge (e.g., limit to 5000 chars for clean output)\n",
    "    if len(sample_text) > 5000:\n",
    "        sample_text = sample_text[:5000]\n",
    "\n",
    "    print(f\"Sample Text Length: {len(sample_text)} characters\")\n",
    "    print(f\"Sample snippet: {sample_text[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching sample: {e}\")\n",
    "    sample_text = \"The quick brown fox jumps over the lazy dog. New York is a big city. \" * 100\n",
    "\n",
    "mw_count = evaluate_tokenizer(mw_tokenizer, sample_text, \"Multi-Word Tokenizer\")\n",
    "gpt2_count = evaluate_tokenizer(gpt2_tokenizer, sample_text, \"GPT-2 BPE (tiktoken)\")\n",
    "\n",
    "diff = gpt2_count - mw_count\n",
    "pct = (diff / gpt2_count) * 100\n",
    "print(f\"\\nDifference: {diff} tokens ({pct:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "isb9v8zLUQ5_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1771100664903,
     "user": {
      "displayName": "Amit Avigad",
      "userId": "05878735534699042778"
     },
     "user_tz": -120
    },
    "id": "isb9v8zLUQ5_",
    "outputId": "fe894a64-efa4-4173-b135-c964e86ffa76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Phrase: 'new york'\n",
      "MW Encoded: [3605, 331, 967]\n",
      "GPT-2 Encoded: [3605, 331, 967]\n"
     ]
    }
   ],
   "source": [
    "# Spot check\n",
    "#sample_phrase = common_phrases[0] if common_phrases else \"test phrase\"\n",
    "sample_phrase = \"new york\"\n",
    "print(f\"Sample Phrase: '{sample_phrase}'\")\n",
    "print(f\"MW Encoded: {mw_tokenizer.encode(sample_phrase)}\")\n",
    "print(f\"GPT-2 Encoded: {gpt2_tokenizer.encode(sample_phrase)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NPAfEPylUQ5_",
   "metadata": {
    "id": "NPAfEPylUQ5_"
   },
   "source": [
    "## 6. Model Implementation (GPT-2)\n",
    "\n",
    "Copying the GPT-2 model architecture from `llm-from-scratch.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "62mNcsagUQ5_",
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1771100664922,
     "user": {
      "displayName": "Amit Avigad",
      "userId": "05878735534699042778"
     },
     "user_tz": -120
    },
    "id": "62mNcsagUQ5_"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccPEGsVgUQ5_",
   "metadata": {
    "id": "ccPEGsVgUQ5_"
   },
   "source": [
    "## 7. Streaming Data Loaders\n",
    "\n",
    "We implement a `StreamingTextDataset` using `IterableDataset` to handle the streaming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "JA654IWLUQ6A",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1771100664934,
     "user": {
      "displayName": "Amit Avigad",
      "userId": "05878735534699042778"
     },
     "user_tz": -120
    },
    "id": "JA654IWLUQ6A"
   },
   "outputs": [],
   "source": [
    "class StreamingTextDataset(IterableDataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, max_length, stride):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.stride = stride\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Iterate over streaming HF dataset\n",
    "        buffer_tokens = []\n",
    "\n",
    "        for doc in self.hf_dataset:\n",
    "            text = doc['text']\n",
    "\n",
    "            # Tokenize\n",
    "            if hasattr(self.tokenizer, \"encode\"):\n",
    "                 try:\n",
    "                     # Use allowed_special for both tokenizers to be safe\n",
    "                     token_ids = self.tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "                 except TypeError:\n",
    "                     token_ids = self.tokenizer.encode(text)\n",
    "            else:\n",
    "                 continue # skip invalid\n",
    "\n",
    "            buffer_tokens.extend(token_ids)\n",
    "\n",
    "            while len(buffer_tokens) >= self.max_length + 1:\n",
    "                chunk = buffer_tokens[:self.max_length + 1]\n",
    "                input_chunk = torch.tensor(chunk[:-1])\n",
    "                target_chunk = torch.tensor(chunk[1:])\n",
    "                yield input_chunk, target_chunk\n",
    "\n",
    "                buffer_tokens = buffer_tokens[self.stride:]\n",
    "\n",
    "def create_streaming_dataloader(hf_dataset, tokenizer, batch_size=4, max_length=256, stride=128):\n",
    "    dataset = StreamingTextDataset(hf_dataset, tokenizer, max_length, stride)\n",
    "    return DataLoader(dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bFUozngUQ6A",
   "metadata": {
    "id": "4bFUozngUQ6A"
   },
   "source": [
    "## 8. Training Loop\n",
    "\n",
    "A training loop adapted for infinite streams (based on steps, not epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6zrs50YxUQ6A",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1771100664944,
     "user": {
      "displayName": "Amit Avigad",
      "userId": "05878735534699042778"
     },
     "user_tz": -120
    },
    "id": "6zrs50YxUQ6A"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, device, max_steps=None, eval_freq=1000):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    step = 0\n",
    "    loss_val = 0.0\n",
    "\n",
    "    # Iterate over stream\n",
    "    for input_batch, target_batch in train_loader:\n",
    "        if max_steps is not None and step >= max_steps:\n",
    "            break\n",
    "\n",
    "        input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_batch)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val = loss.item()\n",
    "\n",
    "        if step % eval_freq == 0:\n",
    "            print(f\"Step {step}, Loss: {loss_val:.4f}\")\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vu56psS1UQ6A",
   "metadata": {
    "id": "vu56psS1UQ6A"
   },
   "source": [
    "## 9. Train Models & Compare\n",
    "\n",
    "We will train two small GPT models. One with standard BPE, one with Multi-Word Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "j3kFcieZUQ6A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1771100665072,
     "user": {
      "displayName": "Amit Avigad",
      "userId": "05878735534699042778"
     },
     "user_tz": -120
    },
    "id": "j3kFcieZUQ6A",
    "outputId": "ebd67363-67cb-4019-9629-fa6afe0c44a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Configuration for small model (to run fast in lab)\n",
    "# Significantly smaller than GPT-2 Small to allow quick training\n",
    "SMALL_GPT_CONFIG = {\n",
    "    \"vocab_size\": 50257,    # Will be updated per tokenizer\n",
    "    \"context_length\": 128,  # Context length\n",
    "    \"emb_dim\": 256,         # Embedding dimension\n",
    "    \"n_heads\": 4,           # Number of attention heads\n",
    "    \"n_layers\": 4,          # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # QKV bias\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bgz7VYlIUQ6A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117,
     "referenced_widgets": [
      "5f06a2cf25e34e1e9a274bc52fd96d2c",
      "dc63c893b51445eb8f55ce293289ad13",
      "e6f1d7e14a6f45c99e5e204634fb1050",
      "403a9d03ffb64dadbf22a117f81e11eb",
      "c54a289295ad400cac4fa3b1e67192cb",
      "3eb67976779e45569655e25ce2ff395c",
      "c8458989aaed48c8b8ea7448392422b6",
      "fa0728164e1542c294cae3696efa34c5",
      "e9c19ec397d84217bdf1b78449fa36a8",
      "f532a1377c33421e802462c203a19196",
      "5bb04de9e3db4865a07cc4cd5e338b10",
      "0558212eb2324fe693156c91ef97e881",
      "ff22adbc934c4b9490b406057862c43d",
      "8c2966981fc345658687d80f435fc9dc",
      "53e0baf8496e48fb89b082a04057c153",
      "c47fa2f38b764d29a530eaf652a2b505",
      "7e77a59b7d054f0884112347725927ce",
      "dc3c2b046ad14dc0b0dc51efcf861369",
      "e3072acee93b42e9a95e099510f28be2",
      "a0981199017e42daae6503d2d12a865a",
      "b9e46d5471174180b82f87e674573381",
      "7f8733235a4d4abb9e1dd1744daaa2ff"
     ]
    },
    "executionInfo": {
     "elapsed": 1588424,
     "status": "ok",
     "timestamp": 1771136105246,
     "user": {
      "displayName": "Amit Avigad",
      "userId": "05878735534699042778"
     },
     "user_tz": -120
    },
    "id": "bgz7VYlIUQ6A",
    "outputId": "98b499c7-8cf4-40a3-944a-f08d1a5d42fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Baseline BPE Model ---\n",
      "Step 0, Loss: 10.9786\n",
      "Step 1000, Loss: 6.8831\n",
      "Step 2000, Loss: 6.7784\n",
      "Step 3000, Loss: 5.7678\n",
      "Step 4000, Loss: 6.4348\n",
      "Step 5000, Loss: 6.5063\n"
     ]
    }
   ],
   "source": [
    "# 1. Baseline Model (BPE)\n",
    "print(\"--- Training Baseline BPE Model ---\")\n",
    "bpe_config = SMALL_GPT_CONFIG.copy()\n",
    "bpe_config[\"vocab_size\"] = 50257 # Standard GPT-2 vocab size\n",
    "\n",
    "bpe_model = GPTModel(bpe_config)\n",
    "bpe_optimizer = torch.optim.AdamW(bpe_model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "\n",
    "# Create NEW streaming dataset iterator for training\n",
    "dataset_bpe = load_dataset(\"Skylion007/openwebtext\", split=\"train\", streaming=True)\n",
    "dataset_bpe = dataset_bpe.shuffle(seed=42, buffer_size=1000)\n",
    "bpe_loader = create_streaming_dataloader(dataset_bpe, gpt2_tokenizer, batch_size=4, max_length=SMALL_GPT_CONFIG[\"context_length\"])\n",
    "\n",
    "bpe_model = train_model(bpe_model, bpe_loader, bpe_optimizer, device, max_steps=6000, eval_freq=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "_O6fsT0oUQ6B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 135,
     "referenced_widgets": [
      "1938bd2b0383418c84559c5068007e9f",
      "8e321513b4504baf8c692bc953f39c7f",
      "e59cf8c0ef4a4616a66ea383736d3059",
      "ad78ec0e35c84970b56d3ebb5f198cc5",
      "f9dae90626d141c0ba2d0b4dd9410a15",
      "f65d6ef03b054ae8a2ca1705be6c2f92",
      "94e8c744ea4b4958a7220443a8a78a7c",
      "ee97e457fbe6433c80b2b793deda9279",
      "76e0620eebe84a5388d7d76875f92d61",
      "be49934bf07542689db28fef3af23d49",
      "ef56ef3a53c0459298de25282d453963",
      "5a5ac78e44724fcc91193bf5ec91cdf6",
      "5885dacea6e2494bb1192e486ced826b",
      "1782a0ff23904fb584a52745c87aa4da",
      "86c2cdf90eeb4f70840a7ece304af08e",
      "ec37a79f3f8b49a9a6b7e260a046c647",
      "3448cd766dfb4174869e841983ca44c7",
      "863371d1efc94aac9a630fcdf2069c18",
      "db294790f40c48ca92d38335c9ce2d93",
      "dd6e88efb5d34ff4b4337f7498650803",
      "01ba8589ff884002a9b034db9fef06fd",
      "3647fe99215a4c308945d4e4805ad117"
     ]
    },
    "executionInfo": {
     "elapsed": 1606141,
     "status": "ok",
     "timestamp": 1771138620604,
     "user": {
      "displayName": "Amit Avigad",
      "userId": "05878735534699042778"
     },
     "user_tz": -120
    },
    "id": "_O6fsT0oUQ6B",
    "outputId": "6c9e5d7e-d2f5-4829-e405-45e96c44fb4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Multi-Word Tokenizer Model ---\n",
      "Step 0, Loss: 10.9875\n",
      "Step 1000, Loss: 7.6526\n",
      "Step 2000, Loss: 7.0121\n",
      "Step 3000, Loss: 6.8444\n",
      "Step 4000, Loss: 6.5920\n",
      "Step 5000, Loss: 5.8412\n"
     ]
    }
   ],
   "source": [
    "# 2. Multi-Word Model\n",
    "print(\"\\n--- Training Multi-Word Tokenizer Model ---\")\n",
    "mw_config = SMALL_GPT_CONFIG.copy()\n",
    "mw_config[\"vocab_size\"] = mw_tokenizer.vocab_size # Use explicit vocab_size\n",
    "\n",
    "mw_model = GPTModel(mw_config)\n",
    "mw_optimizer = torch.optim.AdamW(mw_model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "\n",
    "# Create NEW streaming dataset iterator for training\n",
    "dataset_mw = load_dataset(\"Skylion007/openwebtext\", split=\"train\", streaming=True)\n",
    "dataset_mw = dataset_mw.shuffle(seed=42, buffer_size=1000)\n",
    "mw_loader = create_streaming_dataloader(dataset_mw, mw_tokenizer, batch_size=4, max_length=SMALL_GPT_CONFIG[\"context_length\"])\n",
    "\n",
    "mw_model = train_model(mw_model, mw_loader, mw_optimizer, device, max_steps=6000, eval_freq=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2mzD1B9zUQ6B",
   "metadata": {
    "id": "2mzD1B9zUQ6B"
   },
   "source": [
    "## 10. Qualitative Comparison\n",
    "\n",
    "Generate text from both models to see if there's a difference in coherence (given limited training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6kccEduwUQ6B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 811,
     "status": "ok",
     "timestamp": 1771138621419,
     "user": {
      "displayName": "Amit Avigad",
      "userId": "05878735534699042778"
     },
     "user_tz": -120
    },
    "id": "6kccEduwUQ6B",
    "outputId": "95a67b1c-ed01-4c29-d1a0-0b4723895d06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'I love'\n",
      "\n",
      "--- Baseline BPE Generation (Temp=0.7, TopK=50) ---\n",
      "I love for me up. A lot of my own experience with the best-like, and I was also in the same and it was the rest of the country, and the right of a long of the most important-game. He's no longer I was able to get his son of the most of his mother in the first and I was that I was a great of the first time.\n",
      "\n",
      "She's a number of the time of the same week, when I was an opportunity to know, and I was no longer\u2019s son in their way to know for me of the two of the fact that I\u2019s name.\n",
      "\n",
      "\n",
      "We\u2019s mother in the next year after the \u201cI am you a two-ball and was \u201cThis is really in his way to be in the rest of the other rights of a good and I was taken by both sides\n",
      "\n",
      "\u201cWe were so I was a pretty good of a good-to-up\n",
      "\n",
      "--- Multi-Word Generation (Temp=0.7, TopK=50) ---\n",
      "I love, the anti-free, even and no time.\n",
      "\n",
      "The former two games, the former first-right-right, he knew a left-free is where we're able to keep her.\n",
      "\n",
      "It's really are to identify the ball, including the State\u2019s nowaken\u2019s best, especially to close the opportunity to live the man of State \u201cThe government,\u201d of America.\n",
      "\n",
      "This anything.\n",
      "\n",
      "\u201cS.\u201d\n",
      "\n",
      "The city, it\u201d\n",
      "\n",
      "\"The government is, who is to my first-right, a woman.\n",
      "\n",
      "The Jewish is no, he did some one-year-24% of America, a professor\u2019s.\n",
      "\n",
      "He is a woman.\n",
      "\n",
      "A man\u201d The report.\u201d I\u2019s it.\n",
      "\n",
      "\u201cI am a \u201cThese \u201cthe-right was found the man is.\n",
      "\n",
      "The man was so,\ufffd\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, start_context, max_new_tokens, context_length, device, temperature=0.7, top_k=None):\n",
    "    model.eval()\n",
    "\n",
    "    # Handle encoding variations\n",
    "    if hasattr(tokenizer, \"encode\"):\n",
    "         try:\n",
    "             encoded = tokenizer.encode(start_context, allowed_special={\"<|endoftext|>\"})\n",
    "         except TypeError:\n",
    "             encoded = tokenizer.encode(start_context)\n",
    "\n",
    "    idx = torch.tensor(encoded).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_length:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        \n",
    "        # Apply temperature\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "        \n",
    "        # Apply Top-K filtering\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            probas = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probas, num_samples=1)\n",
    "        else:\n",
    "            # Greedy decoding if temperature is 0\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "            \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return tokenizer.decode(idx.squeeze(0).tolist())\n",
    "\n",
    "start_text = \"I love\"\n",
    "\n",
    "print(f\"Prompt: '{start_text}'\")\n",
    "\n",
    "print(\"\\n--- Baseline BPE Generation (Temp=0.7, TopK=50) ---\")\n",
    "print(generate_text(bpe_model, gpt2_tokenizer, start_text, 200, SMALL_GPT_CONFIG[\"context_length\"], device, temperature=0.7, top_k=50))\n",
    "\n",
    "print(\"\\n--- Multi-Word Generation (Temp=0.7, TopK=50) ---\")\n",
    "print(generate_text(mw_model, mw_tokenizer, start_text, 200, SMALL_GPT_CONFIG[\"context_length\"], device, temperature=0.7, top_k=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce3BG_8UQ6B",
   "metadata": {
    "id": "9ce3BG_8UQ6B"
   },
   "source": [
    "## 11. TriviaQA Evaluation\n",
    "\n",
    "We use the TriviaQA dataset (rc.nocontext) to measure perplexity. Lower perplexity indicates the model is less \"surprised\" by the text, suggesting better language modeling capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ttlUmy8kUQ6B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262,
     "referenced_widgets": [
      "9d40b0a010534326bfc0b8160817ac06",
      "b58bc085f3f644feab7eba69c31bbfd5",
      "84787b659f964f15b532d912d328a147",
      "eb51704546f34d2b895ae381c27fac44",
      "223b84872372426eaf426fd50bf5cbf9",
      "ba63f96e72b74b96a08ab85d85e386a1",
      "c884704337cc410181f56754bf431615",
      "848cc8fb52b44ff9a84d84b121675015",
      "864465aada6a440e8086bef975f298d1",
      "d345cee79d9d4731af6db6caa6139924",
      "85b24a7d5a30493e9ad4b2cacc7f5642",
      "299208a6ad8c4472922fb889d219c9a8",
      "a42692a561954d24a08f183db466dd0c",
      "fdea034ad4a943d9b5dcce01c213d68f",
      "4e8b218821744834904fa5f472f598bc",
      "f21e6f0672dd4a80b69496ef87b4e078",
      "267783af165f4ffcbb3f12295398a22d",
      "05061798dd544ce09cb5ca132ba511a3",
      "1175df8ffe5544cba16ea766afed3764",
      "c2a154ffc0dc465aa20b58194dd9a9ee",
      "10adad2b1e4c40f4b1410a2207deb7fa",
      "f4b5603dad10449eaac39d68f71f1a20"
     ]
    },
    "executionInfo": {
     "elapsed": 8285,
     "status": "ok",
     "timestamp": 1771138629706,
     "user": {
      "displayName": "Amit Avigad",
      "userId": "05878735534699042778"
     },
     "user_tz": -120
    },
    "id": "ttlUmy8kUQ6B",
    "outputId": "f801b243-783c-420b-8c51-c0353feee204"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TriviaQA (rc.nocontext) validation subset...\n",
      "Evaluating on 5000 examples...\n",
      "\n",
      "--- Results ---\n",
      "GPT-2 BPE Average Perplexity: 1872.2816\n",
      "Multi-Word Average Perplexity: 2875.7637\n",
      "\n",
      "Winner: Baseline BPE Model is less surprised by the data.\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(model, tokenizer, text, context_length, device):\n",
    "    model.eval()\n",
    "\n",
    "    if hasattr(tokenizer, \"encode\"):\n",
    "       encodings = tokenizer.encode(text)\n",
    "\n",
    "    if len(encodings) < 2:\n",
    "        return float('nan')\n",
    "\n",
    "    input_ids = torch.tensor(encodings).to(device)\n",
    "\n",
    "    max_len = context_length\n",
    "    stride = max_len\n",
    "\n",
    "    seq_len = input_ids.size(0)\n",
    "    prev_end_loc = 0\n",
    "\n",
    "    nlls = []\n",
    "\n",
    "    for begin_loc in range(0, seq_len - 1, stride):\n",
    "        end_loc = min(begin_loc + max_len, seq_len)\n",
    "\n",
    "        if end_loc - begin_loc < 2:\n",
    "             break\n",
    "\n",
    "        input_chunk = input_ids[begin_loc:end_loc]\n",
    "\n",
    "        inputs = input_chunk[:-1].unsqueeze(0)\n",
    "        targets = input_chunk[1:].unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs)\n",
    "            loss = torch.nn.functional.cross_entropy(logits.permute(0, 2, 1), targets, reduction='sum')\n",
    "            nlls.append(loss.item())\n",
    "\n",
    "    if not nlls:\n",
    "        return float('nan')\n",
    "\n",
    "    total_nll = sum(nlls)\n",
    "    total_tokens_predicted = sum([min(i + max_len, seq_len) - i - 1 for i in range(0, seq_len - 1, stride)])\n",
    "\n",
    "    if total_tokens_predicted <= 0: return float('nan')\n",
    "\n",
    "    return torch.exp(torch.tensor(total_nll / total_tokens_predicted))\n",
    "\n",
    "print(\"Loading TriviaQA (rc.nocontext) validation subset...\")\n",
    "trivia_data = load_dataset(\"trivia_qa\", \"rc.nocontext\", split=\"validation\", streaming=True)\n",
    "\n",
    "# Evaluate on first 5000 samples\n",
    "eval_limit = 5000\n",
    "bpe_ppls = []\n",
    "mw_ppls = []\n",
    "\n",
    "print(f\"Evaluating on {eval_limit} examples...\")\n",
    "\n",
    "for i, item in enumerate(trivia_data):\n",
    "    if i >= eval_limit: break\n",
    "\n",
    "    question = item['question']\n",
    "    text = f\"Question: {question}\\nAnswer:\"\n",
    "\n",
    "    ppl_bpe = calculate_perplexity(bpe_model, gpt2_tokenizer, text, SMALL_GPT_CONFIG[\"context_length\"], device)\n",
    "    ppl_mw = calculate_perplexity(mw_model, mw_tokenizer, text, SMALL_GPT_CONFIG[\"context_length\"], device)\n",
    "\n",
    "    if not torch.isnan(ppl_bpe): bpe_ppls.append(ppl_bpe)\n",
    "    if not torch.isnan(ppl_mw): mw_ppls.append(ppl_mw)\n",
    "\n",
    "avg_bpe_ppl = torch.tensor(bpe_ppls).mean().item()\n",
    "avg_mw_ppl = torch.tensor(mw_ppls).mean().item()\n",
    "\n",
    "print(f\"\\n--- Results ---\")\n",
    "print(f\"GPT-2 BPE Average Perplexity: {avg_bpe_ppl:.4f}\")\n",
    "print(f\"Multi-Word Average Perplexity: {avg_mw_ppl:.4f}\")\n",
    "\n",
    "if avg_mw_ppl < avg_bpe_ppl:\n",
    "    print(\"\\nWinner: Multi-Word Tokenizer Model is less surprised by the data.\")\n",
    "else:\n",
    "    print(\"\\nWinner: Baseline BPE Model is less surprised by the data.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}